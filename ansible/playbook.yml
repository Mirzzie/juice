---
- name: Configure EC2 for Juice Shop with DevSecOps Tools + Monitoring
  hosts: juiceshop
  become: true
  gather_facts: true

  vars:
    juice_shop_dir: /opt/juice-shop
    metrics_dir: /opt/security-metrics
    node_version: "22"
    prometheus_version: "2.48.0"

  vars_files:
    - secrets.yml

  tasks:
    # =======================================================================
    # SYSTEM PREPARATION
    # =======================================================================
    - name: Update APT cache
      apt:
        update_cache: yes
        cache_valid_time: 3600

    - name: Install required system packages
      apt:
        name:
          - curl
          - wget
          - git
          - unzip
          - software-properties-common
          - apt-transport-https
          - ca-certificates
          - gnupg
          - lsb-release
          - jq
          - python3
          - python3-pip
          - python3-venv
        state: present

    # =======================================================================
    # DOCKER INSTALLATION
    # =======================================================================
    - name: Add Docker GPG key
      apt_key:
        url: https://download.docker.com/linux/ubuntu/gpg
        state: present

    - name: Add Docker repository
      apt_repository:
        repo: "deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable"
        state: present

    - name: Install Docker
      apt:
        name:
          - docker-ce
          - docker-ce-cli
          - containerd.io
          - docker-buildx-plugin
          - docker-compose-plugin
        state: present
        update_cache: yes

    - name: Add ubuntu user to docker group
      user:
        name: ubuntu
        groups: docker
        append: yes

    - name: Start and enable Docker service
      systemd:
        name: docker
        state: started
        enabled: yes

    # =======================================================================
    # DATADOG AGENT INSTALLATION
    # =======================================================================
    - name: Check if DataDog agent is already installed
      stat:
        path: /etc/datadog-agent
      register: datadog_installed

    - name: Download DataDog installation script
      get_url:
        url: "https://install.datadoghq.com/scripts/install_script_agent7.sh"
        dest: /tmp/install_datadog.sh
        mode: '0755'
      when: not datadog_installed.stat.exists

    - name: Install DataDog Agent
      shell: >
        DD_API_KEY={{ datadog_api_key }} DD_SITE={{ datadog_site }} bash /tmp/install_datadog.sh
      args:
        creates: /etc/datadog-agent/datadog.yaml
      when: not datadog_installed.stat.exists
      register: datadog_install
      retries: 3
      delay: 15
      until: datadog_install.rc == 0

    - name: Configure DataDog Agent for APM and IAST
      blockinfile:
        path: /etc/datadog-agent/datadog.yaml
        block: |
          apm_config:
            enabled: true
            apm_non_local_traffic: true
            receiver_port: 8126
            receiver_timeout: 10
            max_traces_per_second: 100

          appsec_config:
            enabled: true

          runtime_security_config:
            enabled: true

          process_config:
            enabled: "true"
            scrub_args: true

          logs_enabled: true
          logs_config:
            container_collect_all: true
        marker: "# {mark} ANSIBLE MANAGED BLOCK - DEVSECOPS CONFIG"
        create: no
        backup: yes
      notify: Restart DataDog Agent

    - name: Ensure DataDog Agent is running
      systemd:
        name: datadog-agent
        state: started
        enabled: yes

    # =======================================================================
    # PROMETHEUS INSTALLATION
    # =======================================================================
    - name: Create Prometheus user
      user:
        name: prometheus
        system: yes
        shell: /bin/false
        create_home: no

    - name: Create Prometheus directories
      file:
        path: "{{ item }}"
        state: directory
        owner: prometheus
        group: prometheus
        mode: '0755'
      loop:
        - /etc/prometheus
        - /var/lib/prometheus

    - name: Download and extract Prometheus
      unarchive:
        src: "https://github.com/prometheus/prometheus/releases/download/v{{ prometheus_version }}/prometheus-{{ prometheus_version }}.linux-amd64.tar.gz"
        dest: /tmp
        remote_src: yes
        creates: /tmp/prometheus-{{ prometheus_version }}.linux-amd64

    - name: Copy Prometheus binaries
      copy:
        src: "/tmp/prometheus-{{ prometheus_version }}.linux-amd64/{{ item }}"
        dest: /usr/local/bin/
        mode: '0755'
        owner: prometheus
        group: prometheus
        remote_src: yes
      loop:
        - prometheus
        - promtool

    - name: Create Prometheus configuration
      copy:
        dest: /etc/prometheus/prometheus.yml
        content: |
          global:
            scrape_interval: 15s
            evaluation_interval: 15s

          scrape_configs:
            - job_name: 'prometheus'
              static_configs:
                - targets: ['localhost:9090']

            - job_name: 'security-metrics-exporter'
              static_configs:
                - targets: ['localhost:9999']
              scrape_interval: 30s
        owner: prometheus
        group: prometheus
        mode: '0644'
      notify: Restart Prometheus

    - name: Create Prometheus systemd service
      copy:
        dest: /etc/systemd/system/prometheus.service
        content: |
          [Unit]
          Description=Prometheus
          Wants=network-online.target
          After=network-online.target

          [Service]
          User=prometheus
          Group=prometheus
          Type=simple
          ExecStart=/usr/local/bin/prometheus \
            --config.file=/etc/prometheus/prometheus.yml \
            --storage.tsdb.path=/var/lib/prometheus/ \
            --web.console.templates=/etc/prometheus/consoles \
            --web.console.libraries=/etc/prometheus/console_libraries

          [Install]
          WantedBy=multi-user.target
        mode: '0644'
      notify: Restart Prometheus

    - name: Enable and start Prometheus
      systemd:
        name: prometheus
        enabled: yes
        state: started
        daemon_reload: yes

    # =======================================================================
    # GRAFANA INSTALLATION
    # =======================================================================
    - name: Add Grafana GPG key
      apt_key:
        url: https://apt.grafana.com/gpg.key
        state: present

    - name: Add Grafana repository
      apt_repository:
        repo: "deb https://apt.grafana.com stable main"
        state: present

    - name: Install Grafana
      apt:
        name: grafana
        state: present
        update_cache: yes

    - name: Configure Grafana to run on port 3001
      lineinfile:
        path: /etc/grafana/grafana.ini
        regexp: '^;?http_port ='
        line: 'http_port = 3001'
        state: present
      notify: Restart Grafana

    - name: Create Grafana provisioning directories
      file:
        path: "/etc/grafana/provisioning/{{ item }}"
        state: directory
        owner: grafana
        group: grafana
        mode: '0755'
      loop:
        - datasources
        - dashboards

    - name: Configure Prometheus datasource for Grafana
      copy:
        dest: /etc/grafana/provisioning/datasources/prometheus.yml
        content: |
          apiVersion: 1
          datasources:
            - name: Prometheus
              type: prometheus
              access: proxy
              url: http://localhost:9090
              isDefault: true
              editable: false
        owner: grafana
        group: grafana
        mode: '0644'
      notify: Restart Grafana

    - name: Create dashboard provisioning config
      copy:
        dest: /etc/grafana/provisioning/dashboards/default.yml
        content: |
          apiVersion: 1
          providers:
            - name: 'Default'
              orgId: 1
              folder: ''
              type: file
              disableDeletion: false
              updateIntervalSeconds: 10
              allowUiUpdates: true
              options:
                path: /var/lib/grafana/dashboards
        owner: grafana
        group: grafana
        mode: '0644'
      notify: Restart Grafana

    - name: Create Grafana dashboards directory
      file:
        path: /var/lib/grafana/dashboards
        state: directory
        owner: grafana
        group: grafana
        mode: '0755'

    - name: Create Enhanced SAST/DAST/IAST/RASP Comparison Dashboard
      copy:
        dest: /var/lib/grafana/dashboards/security-comparison.json
        content: |
          {
            "dashboard": {
              "title": "DevSecOps Security Tools Comparison (SAST/DAST/IAST/RASP)",
              "tags": ["security", "devsecops", "sast", "dast", "iast", "rasp"],
              "timezone": "browser",
              "refresh": "30s",
              "panels": [
                {
                  "id": 1,
                  "title": "üìä Overall Security Findings",
                  "type": "bargauge",
                  "gridPos": {"h": 8, "w": 24, "x": 0, "y": 0},
                  "targets": [
                    {
                      "expr": "sum(sast_vulnerabilities_found{rule_id=\"total\"})",
                      "legendFormat": "SAST (Semgrep) - Code Analysis"
                    },
                    {
                      "expr": "sum(dast_vulnerabilities_found{vuln_name=\"total\", scan_type=\"full\"})",
                      "legendFormat": "DAST (ZAP) - Dynamic Testing"
                    },
                    {
                      "expr": "sum(security_detections_total{tool=\"datadog_iast\"})",
                      "legendFormat": "IAST (DataDog) - Runtime Detection"
                    },
                    {
                      "expr": "sum(security_detections_total{tool=\"aikido_rasp\"})",
                      "legendFormat": "RASP (Aikido) - Runtime Detection"
                    }
                  ],
                  "options": {
                    "orientation": "horizontal",
                    "displayMode": "gradient",
                    "showUnfilled": true
                  },
                  "fieldConfig": {
                    "defaults": {
                      "thresholds": {
                        "mode": "absolute",
                        "steps": [
                          {"value": 0, "color": "green"},
                          {"value": 10, "color": "yellow"},
                          {"value": 25, "color": "orange"},
                          {"value": 50, "color": "red"}
                        ]
                      },
                      "unit": "short"
                    }
                  }
                },
                {
                  "id": 2,
                  "title": "üõ°Ô∏è RASP Attack Blocking (Real-time Protection)",
                  "type": "stat",
                  "gridPos": {"h": 6, "w": 12, "x": 0, "y": 8},
                  "targets": [
                    {
                      "expr": "sum(security_blocks_total{tool=\"aikido_rasp\"})",
                      "legendFormat": "Attacks Blocked"
                    }
                  ],
                  "options": {
                    "colorMode": "background",
                    "graphMode": "area",
                    "justifyMode": "center"
                  },
                  "fieldConfig": {
                    "defaults": {
                      "thresholds": {
                        "mode": "absolute",
                        "steps": [
                          {"value": 0, "color": "red"},
                          {"value": 10, "color": "orange"},
                          {"value": 30, "color": "green"}
                        ]
                      },
                      "unit": "short"
                    }
                  }
                },
                {
                  "id": 3,
                  "title": "üéØ RASP Block Rate (%)",
                  "type": "gauge",
                  "gridPos": {"h": 6, "w": 12, "x": 12, "y": 8},
                  "targets": [
                    {
                      "expr": "(sum(security_blocks_total{tool=\"aikido_rasp\"}) / sum(security_detections_total{tool=\"aikido_rasp\"})) * 100",
                      "legendFormat": "Block Rate"
                    }
                  ],
                  "options": {
                    "showThresholdLabels": true,
                    "showThresholdMarkers": true
                  },
                  "fieldConfig": {
                    "defaults": {
                      "thresholds": {
                        "mode": "absolute",
                        "steps": [
                          {"value": 0, "color": "red"},
                          {"value": 50, "color": "yellow"},
                          {"value": 75, "color": "green"}
                        ]
                      },
                      "unit": "percent",
                      "min": 0,
                      "max": 100
                    }
                  }
                },
                {
                  "id": 4,
                  "title": "üîç SAST Findings (Semgrep) - By Severity",
                  "type": "piechart",
                  "gridPos": {"h": 8, "w": 12, "x": 0, "y": 14},
                  "targets": [
                    {
                      "expr": "sum by (severity) (sast_vulnerabilities_found{rule_id=\"total\"})",
                      "legendFormat": "{{severity}}"
                    }
                  ]
                },
                {
                  "id": 5,
                  "title": "üï∑Ô∏è DAST Vulnerabilities (ZAP) - By Severity",
                  "type": "piechart",
                  "gridPos": {"h": 8, "w": 12, "x": 12, "y": 14},
                  "targets": [
                    {
                      "expr": "sum by (severity) (dast_vulnerabilities_found{vuln_name=\"total\", scan_type=\"full\"})",
                      "legendFormat": "{{severity}}"
                    }
                  ]
                },
                {
                  "id": 6,
                  "title": "üìà IAST vs RASP Detection Timeline",
                  "type": "timeseries",
                  "gridPos": {"h": 8, "w": 24, "x": 0, "y": 22},
                  "targets": [
                    {
                      "expr": "rate(security_detections_total{tool=\"datadog_iast\"}[5m])",
                      "legendFormat": "IAST Detection Rate (per min)"
                    },
                    {
                      "expr": "rate(security_detections_total{tool=\"aikido_rasp\"}[5m])",
                      "legendFormat": "RASP Detection Rate (per min)"
                    },
                    {
                      "expr": "rate(security_blocks_total{tool=\"aikido_rasp\"}[5m])",
                      "legendFormat": "RASP Block Rate (per min)"
                    }
                  ],
                  "fieldConfig": {
                    "defaults": {
                      "custom": {
                        "drawStyle": "line",
                        "lineInterpolation": "smooth",
                        "fillOpacity": 10
                      }
                    }
                  }
                },
                {
                  "id": 7,
                  "title": "üî¨ IAST Detections by Vulnerability Type",
                  "type": "bargauge",
                  "gridPos": {"h": 8, "w": 12, "x": 0, "y": 30},
                  "targets": [
                    {
                      "expr": "sum by (vuln_type) (security_detections_total{tool=\"datadog_iast\"})",
                      "legendFormat": "{{vuln_type}}"
                    }
                  ],
                  "options": {
                    "orientation": "horizontal"
                  }
                },
                {
                  "id": 8,
                  "title": "üõ°Ô∏è RASP Detections by Attack Type",
                  "type": "bargauge",
                  "gridPos": {"h": 8, "w": 12, "x": 12, "y": 30},
                  "targets": [
                    {
                      "expr": "sum by (vuln_type) (security_detections_total{tool=\"aikido_rasp\"})",
                      "legendFormat": "{{vuln_type}}"
                    }
                  ],
                  "options": {
                    "orientation": "horizontal"
                  }
                },
                {
                  "id": 9,
                  "title": "üìã Top 10 DAST Vulnerabilities (Detailed)",
                  "type": "table",
                  "gridPos": {"h": 10, "w": 24, "x": 0, "y": 38},
                  "targets": [
                    {
                      "expr": "topk(10, dast_vulnerabilities_found{vuln_name!=\"total\", scan_type=\"full\"})",
                      "format": "table",
                      "instant": true
                    }
                  ],
                  "transformations": [
                    {
                      "id": "organize",
                      "options": {
                        "excludeByName": {"Time": true, "__name__": true, "scan_type": false},
                        "renameByName": {
                          "vuln_name": "Vulnerability",
                          "severity": "Severity",
                          "confidence": "Confidence",
                          "Value": "Instances Found"
                        }
                      }
                    }
                  ]
                },
                {
                  "id": 10,
                  "title": "‚ö° Current Scan Phase",
                  "type": "stat",
                  "gridPos": {"h": 4, "w": 8, "x": 0, "y": 48},
                  "targets": [
                    {
                      "expr": "current_scan_phase",
                      "legendFormat": "Phase"
                    }
                  ],
                  "options": {
                    "colorMode": "background"
                  },
                  "fieldConfig": {
                    "defaults": {
                      "mappings": [
                        {"type": "value", "value": 0, "text": "Idle"},
                        {"type": "value", "value": 1, "text": "Phase 1: Baseline"},
                        {"type": "value", "value": 2, "text": "Phase 2: Full Scan"},
                        {"type": "value", "value": 3, "text": "Phase 3: Attacks"},
                        {"type": "value", "value": 4, "text": "Phase 4: Analysis"}
                      ]
                    }
                  }
                },
                {
                  "id": 11,
                  "title": "üîÑ DAST: Baseline vs Full Scan",
                  "type": "bargauge",
                  "gridPos": {"h": 4, "w": 16, "x": 8, "y": 48},
                  "targets": [
                    {
                      "expr": "sum(dast_vulnerabilities_found{scan_type=\"baseline\", vuln_name=\"total\"})",
                      "legendFormat": "Baseline Scan"
                    },
                    {
                      "expr": "sum(dast_vulnerabilities_found{scan_type=\"full\", vuln_name=\"total\"})",
                      "legendFormat": "Full Scan"
                    }
                  ]
                },
                {
                  "id": 12,
                  "title": "üéØ Key Thesis Metrics",
                  "type": "stat",
                  "gridPos": {"h": 6, "w": 24, "x": 0, "y": 52},
                  "targets": [
                    {
                      "expr": "sum(sast_vulnerabilities_found{rule_id=\"total\", severity=\"ERROR\"})",
                      "legendFormat": "SAST Critical"
                    },
                    {
                      "expr": "sum(dast_vulnerabilities_found{severity=\"High\", vuln_name=\"total\"})",
                      "legendFormat": "DAST High Risk"
                    },
                    {
                      "expr": "sum(security_detections_total{tool=\"datadog_iast\"})",
                      "legendFormat": "IAST Total"
                    },
                    {
                      "expr": "sum(security_detections_total{tool=\"aikido_rasp\"})",
                      "legendFormat": "RASP Detected"
                    },
                    {
                      "expr": "sum(security_blocks_total{tool=\"aikido_rasp\"})",
                      "legendFormat": "RASP Blocked"
                    },
                    {
                      "expr": "(sum(security_blocks_total{tool=\"aikido_rasp\"}) / sum(security_detections_total{tool=\"aikido_rasp\"})) * 100",
                      "legendFormat": "RASP Block %"
                    }
                  ],
                  "options": {
                    "colorMode": "value",
                    "graphMode": "none",
                    "justifyMode": "auto",
                    "orientation": "horizontal"
                  }
                }
              ],
              "schemaVersion": 36,
              "version": 1,
              "uid": "security-comparison",
              "editable": true
            }
          }
        owner: grafana
        group: grafana
        mode: '0644'
      notify: Restart Grafana

    - name: Enable and start Grafana
      systemd:
        name: grafana-server
        enabled: yes
        state: started

    # =======================================================================
    # METRICS EXPORTER SETUP
    # =======================================================================
    - name: Create metrics directory
      file:
        path: "{{ metrics_dir }}"
        state: directory
        owner: ubuntu
        group: ubuntu
        mode: '0755'

    - name: Create ZAP results directory
      file:
        path: "{{ metrics_dir }}/zap-results"
        state: directory
        owner: ubuntu
        group: ubuntu
        mode: '0755'

    - name: Create metrics data directory
      file:
        path: "{{ metrics_dir }}/data"
        state: directory
        owner: ubuntu
        group: ubuntu
        mode: '0755'

    - name: Create Python virtual environment for metrics exporter
      command: python3 -m venv {{ metrics_dir }}/venv
      args:
        creates: "{{ metrics_dir }}/venv"
      become_user: ubuntu

    - name: Install Python dependencies for metrics exporter
      pip:
        name:
          - prometheus_client
          - requests
          - flask
        virtualenv: "{{ metrics_dir }}/venv"
      become_user: ubuntu

    - name: Create Enhanced Metrics Exporter Script
      copy:
        dest: "{{ metrics_dir }}/metrics_exporter.py"
        content: |
          #!/usr/bin/env python3
          """
          Enhanced Security Metrics Exporter for Prometheus
          Collects metrics from Semgrep SAST, OWASP ZAP DAST, DataDog IAST, and Aikido RASP
          """
          import json
          import os
          import time
          import traceback
          from collections import defaultdict
          from prometheus_client import start_http_server, Gauge, Counter
          import requests

          # ============= PROMETHEUS METRICS DEFINITIONS =============
          
          # SAST Metrics (Semgrep)
          sast_vulnerabilities = Gauge(
              'sast_vulnerabilities_found', 
              'SAST vulnerabilities from Semgrep', 
              ['severity', 'rule_id', 'language']
          )
          
          # DAST Metrics (OWASP ZAP)
          dast_vulnerabilities = Gauge(
              'dast_vulnerabilities_found', 
              'DAST vulnerabilities from ZAP', 
              ['severity', 'scan_type', 'vuln_name', 'confidence']
          )
          
          # IAST/RASP Metrics
          security_detections = Counter(
              'security_detections_total', 
              'Total security detections', 
              ['tool', 'vuln_type', 'severity']
          )
          
          security_blocks = Counter(
              'security_blocks_total', 
              'Total attack blocks (RASP only)', 
              ['tool', 'attack_type']
          )
          
          response_time = Gauge(
              'security_response_time_ms', 
              'Security tool response time impact', 
              ['tool']
          )
          
          scan_phase = Gauge(
              'current_scan_phase', 
              'Current DAST scan phase (0=idle, 1=baseline, 2=full, 3=attacks, 4=analysis)'
          )

          # ============= CONFIGURATION =============
          
          DATADOG_API_KEY = os.environ.get('DD_API_KEY', '')
          DATADOG_APP_KEY = os.environ.get('DD_APP_KEY', '')
          DATADOG_SITE = os.environ.get('DD_SITE', 'us5.datadoghq.com')
          AIKIDO_TOKEN = os.environ.get('AIKIDO_TOKEN', '')
          METRICS_DIR = '/opt/security-metrics/data'
          ZAP_RESULTS_DIR = '/opt/security-metrics/zap-results'

          os.makedirs(METRICS_DIR, exist_ok=True)
          os.makedirs(ZAP_RESULTS_DIR, exist_ok=True)

          # ============= SEMGREP SAST PARSER =============
          
          def parse_semgrep_results():
              """Parse Semgrep SAST results"""
              try:
                  semgrep_file = os.path.join(METRICS_DIR, 'semgrep-results.json')
                  if not os.path.exists(semgrep_file):
                      print(f"‚ö†Ô∏è  Semgrep results not found at {semgrep_file}")
                      return
                  
                  print(f"üìÑ Parsing Semgrep SAST results...")
                  with open(semgrep_file, 'r') as f:
                      data = json.load(f)
                  
                  # Clear old metrics
                  sast_vulnerabilities._metrics.clear()
                  
                  results = data.get('results', [])
                  severity_counts = defaultdict(int)
                  
                  for finding in results:
                      extra = finding.get('extra', {})
                      severity = extra.get('severity', 'INFO').upper()
                      rule_id = finding.get('check_id', 'unknown').split('.')[-1]  # Get short name
                      
                      # Get language from path
                      path = finding.get('path', '')
                      if path.endswith('.js') or path.endswith('.ts'):
                          language = 'javascript'
                      elif path.endswith('.py'):
                          language = 'python'
                      elif path.endswith('.java'):
                          language = 'java'
                      else:
                          language = 'other'
                      
                      severity_counts[severity] += 1
                      
                      # Set detailed metric
                      sast_vulnerabilities.labels(
                          severity=severity,
                          rule_id=rule_id,
                          language=language
                      ).set(1)
                  
                  # Set aggregate counts
                  for severity, count in severity_counts.items():
                      sast_vulnerabilities.labels(
                          severity=severity,
                          rule_id='total',
                          language='all'
                      ).set(count)
                  
                  print(f"‚úÖ SAST (Semgrep): {len(results)} findings")
                  print(f"   - ERROR: {severity_counts.get('ERROR', 0)}")
                  print(f"   - WARNING: {severity_counts.get('WARNING', 0)}")
                  print(f"   - INFO: {severity_counts.get('INFO', 0)}")
                  
              except Exception as e:
                  print(f"‚ùå Error parsing Semgrep results: {e}")
                  traceback.print_exc()

          # ============= OWASP ZAP DAST PARSER =============
          
          def parse_zap_results():
              """Parse OWASP ZAP DAST scan results"""
              try:
                  # Clear old metrics
                  dast_vulnerabilities._metrics.clear()
                  
                  for scan_type in ['baseline', 'full']:
                      json_file = os.path.join(ZAP_RESULTS_DIR, f'{scan_type}_report.json')
                      if not os.path.exists(json_file):
                          print(f"‚ö†Ô∏è  {scan_type} scan results not found at {json_file}")
                          continue
                      
                      print(f"üìÑ Parsing ZAP {scan_type} scan results...")
                      with open(json_file, 'r') as f:
                          data = json.load(f)
                      
                      # Handle both ZAP JSON formats
                      site = data.get('site', [{}])
                      if isinstance(site, list):
                          site = site[0] if len(site) > 0 else {}
                      
                      alerts = site.get('alerts', [])
                      
                      if not alerts:
                          print(f"‚ö†Ô∏è  No alerts found in {scan_type} scan")
                          continue
                      
                      severity_counts = defaultdict(int)
                      
                      for alert in alerts:
                          # Parse risk level
                          risk_desc = alert.get('riskdesc', 'Unknown')
                          risk = risk_desc.split()[0] if ' ' in risk_desc else risk_desc
                          
                          # Get vulnerability details
                          vuln_name = alert.get('name', 'Unknown')
                          confidence = alert.get('confidence', 'Unknown')
                          instances = alert.get('instances', [])
                          instance_count = len(instances)
                          
                          severity_counts[risk] += instance_count
                          
                          # Set detailed metric for each vulnerability
                          dast_vulnerabilities.labels(
                              severity=risk,
                              scan_type=scan_type,
                              vuln_name=vuln_name,
                              confidence=confidence
                          ).set(instance_count)
                      
                      # Set aggregate totals
                      for severity, count in severity_counts.items():
                          dast_vulnerabilities.labels(
                              severity=severity,
                              scan_type=scan_type,
                              vuln_name='total',
                              confidence='all'
                          ).set(count)
                      
                      total = sum(severity_counts.values())
                      print(f"‚úÖ DAST {scan_type.upper()}: {total} vulnerabilities")
                      print(f"   - High: {severity_counts.get('High', 0)}")
                      print(f"   - Medium: {severity_counts.get('Medium', 0)}")
                      print(f"   - Low: {severity_counts.get('Low', 0)}")
                      print(f"   - Info: {severity_counts.get('Informational', 0)}")
                  
              except Exception as e:
                  print(f"‚ùå Error parsing ZAP results: {e}")
                  traceback.print_exc()

          # ============= DATADOG IAST INTEGRATION =============
          
          def fetch_datadog_iast_metrics():
              """Fetch IAST vulnerabilities from DataDog API"""
              if not DATADOG_API_KEY or not DATADOG_APP_KEY:
                  print("‚ö†Ô∏è  DataDog credentials not configured")
                  return

              try:
                  headers = {
                      'DD-API-KEY': DATADOG_API_KEY,
                      'DD-APPLICATION-KEY': DATADOG_APP_KEY,
                      'Content-Type': 'application/json'
                  }
                  
                  # Query security signals for juice-shop service
                  url = f'https://api.{DATADOG_SITE}/api/v2/security_monitoring/signals'
                  params = {
                      'filter[query]': 'service:juice-shop',
                      'filter[from]': int((time.time() - 3600) * 1000),  # Last hour
                      'page[limit]': 100
                  }
                  
                  response = requests.get(url, headers=headers, params=params, timeout=15)
                  
                  if response.status_code == 200:
                      data = response.json()
                      signals = data.get('data', [])
                      detections = defaultdict(int)
                      
                      for signal in signals:
                          attributes = signal.get('attributes', {})
                          rule = attributes.get('rule', {})
                          vuln_type = rule.get('name', 'unknown').lower().replace(' ', '_')
                          severity = attributes.get('severity', 'unknown')
                          
                          detections[(vuln_type, severity)] += 1
                      
                      # Update metrics
                      for (vuln_type, severity), count in detections.items():
                          security_detections.labels(
                              tool='datadog_iast',
                              vuln_type=vuln_type,
                              severity=severity
                          )._value._value = count
                      
                      print(f"‚úÖ DataDog IAST: {len(detections)} vulnerability types, {sum(detections.values())} total detections")
                      
                  elif response.status_code == 403:
                      print(f"‚ùå DataDog API authentication failed - check your API keys")
                  else:
                      print(f"‚ö†Ô∏è  DataDog API returned status {response.status_code}")
                      print(f"Response: {response.text[:200]}")
                      
              except requests.exceptions.Timeout:
                  print(f"‚ö†Ô∏è  DataDog API request timed out")
              except Exception as e:
                  print(f"‚ùå Error fetching DataDog metrics: {e}")

          # ============= AIKIDO RASP INTEGRATION =============
          
          def fetch_aikido_rasp_metrics():
              """Fetch RASP detections and blocks from Aikido API"""
              if not AIKIDO_TOKEN:
                  print("‚ö†Ô∏è  Aikido token not configured")
                  return

              try:
                  headers = {
                      'Authorization': f'Bearer {AIKIDO_TOKEN}',
                      'Content-Type': 'application/json'
                  }
                  
                  # Try to fetch events/attacks from Aikido
                  url = 'https://app.aikido.dev/api/v1/runtime/events'
                  params = {
                      'limit': 100,
                      'from': int((time.time() - 3600) * 1000)  # Last hour
                  }
                  
                  response = requests.get(url, headers=headers, params=params, timeout=15)
                  
                  if response.status_code == 200:
                      data = response.json()
                      events = data.get('events', data.get('data', []))
                      
                      detections = defaultdict(int)
                      blocks = defaultdict(int)
                      
                      for event in events:
                          attack_type = event.get('type', event.get('attack_type', 'unknown'))
                          severity = event.get('severity', 'unknown')
                          blocked = event.get('blocked', event.get('was_blocked', False))
                          
                          detections[(attack_type, severity)] += 1
                          if blocked:
                              blocks[attack_type] += 1
                      
                      # Update detection metrics
                      for (attack_type, severity), count in detections.items():
                          security_detections.labels(
                              tool='aikido_rasp',
                              vuln_type=attack_type,
                              severity=severity
                          )._value._value = count
                      
                      # Update block metrics
                      for attack_type, count in blocks.items():
                          security_blocks.labels(
                              tool='aikido_rasp',
                              attack_type=attack_type
                          )._value._value = count
                      
                      total_detections = sum(detections.values())
                      total_blocks = sum(blocks.values())
                      block_rate = (total_blocks / total_detections * 100) if total_detections > 0 else 0
                      
                      print(f"‚úÖ Aikido RASP: {total_detections} detections, {total_blocks} blocks ({block_rate:.1f}% block rate)")
                      
                  elif response.status_code == 401 or response.status_code == 403:
                      print(f"‚ùå Aikido API authentication failed - check your token")
                  else:
                      print(f"‚ö†Ô∏è  Aikido API returned status {response.status_code}")
                      print(f"Response: {response.text[:200]}")
                      
              except requests.exceptions.Timeout:
                  print(f"‚ö†Ô∏è  Aikido API request timed out")
              except Exception as e:
                  print(f"‚ùå Error fetching Aikido metrics: {e}")

          # ============= MANUAL METRICS LOADER =============
          
          def load_manual_metrics():
              """Load manually recorded metrics from JSON files"""
              try:
                  metrics_file = os.path.join(METRICS_DIR, 'manual_metrics.json')
                  if os.path.exists(metrics_file):
                      with open(metrics_file, 'r') as f:
                          data = json.load(f)
                      
                      if 'scan_phase' in data:
                          scan_phase.set(data['scan_phase'])
                      
                      if 'response_times' in data:
                          for tool, time_ms in data['response_times'].items():
                              response_time.labels(tool=tool).set(time_ms)
                      
                      print("‚úÖ Loaded manual metrics")
              except Exception as e:
                  print(f"‚ö†Ô∏è  Error loading manual metrics: {e}")

          # ============= MAIN COLLECTION LOOP =============
          
          def collect_all_metrics():
              """Collect metrics from all sources"""
              print(f"\n{'='*60}")
              print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] Collecting Security Metrics")
              print(f"{'='*60}")
              
              parse_semgrep_results()      # SAST
              parse_zap_results()          # DAST
              fetch_datadog_iast_metrics() # IAST
              fetch_aikido_rasp_metrics()  # RASP
              load_manual_metrics()        # Manual metrics
              
              print(f"{'='*60}")
              print("‚úÖ Metrics collection cycle complete\n")

          # ============= STARTUP =============
          
          if __name__ == '__main__':
              print("\n" + "="*60)
              print("üöÄ Security Metrics Exporter Starting")
              print("="*60)
              print(f"üìä Metrics endpoint: http://localhost:9999/metrics")
              print(f"üìÅ Metrics directory: {METRICS_DIR}")
              print(f"üï∑Ô∏è  ZAP results directory: {ZAP_RESULTS_DIR}")
              print(f"üîÑ Collection interval: 30 seconds")
              print("="*60 + "\n")
              
              # Start Prometheus HTTP server
              start_http_server(9999)
              print("‚úÖ Prometheus exporter running on port 9999\n")
              
              # Initial collection
              try:
                  collect_all_metrics()
              except Exception as e:
                  print(f"‚ùå Error in initial collection: {e}")
              
              # Continuous collection loop
              while True:
                  try:
                      time.sleep(30)
                      collect_all_metrics()
                  except KeyboardInterrupt:
                      print("\nüëã Shutting down metrics exporter...")
                      break
                  except Exception as e:
                      print(f"‚ùå Error in metrics collection: {e}")
                      traceback.print_exc()
                      time.sleep(30)
        owner: ubuntu
        group: ubuntu
        mode: '0755'

    - name: Create metrics exporter systemd service
      copy:
        dest: /etc/systemd/system/security-metrics-exporter.service
        content: |
          [Unit]
          Description=Security Metrics Exporter for Prometheus (SAST/DAST/IAST/RASP)
          After=network.target prometheus.service

          [Service]
          Type=simple
          User=ubuntu
          WorkingDirectory={{ metrics_dir }}
          Environment="DD_API_KEY={{ datadog_api_key }}"
          Environment="DD_APP_KEY={{ datadog_app_key }}"
          Environment="DD_SITE={{ datadog_site }}"
          Environment="AIKIDO_TOKEN={{ aikido_token }}"
          ExecStart={{ metrics_dir }}/venv/bin/python {{ metrics_dir }}/metrics_exporter.py
          Restart=always
          RestartSec=10
          StandardOutput=journal
          StandardError=journal

          [Install]
          WantedBy=multi-user.target
        mode: '0644'
      notify: Restart Metrics Exporter

    - name: Enable and start metrics exporter
      systemd:
        name: security-metrics-exporter
        enabled: yes
        state: started
        daemon_reload: yes

    # =======================================================================
    # APPLICATION SETUP (YOUR WORKING VERSION)
    # =======================================================================
    - name: Create application directory
      file:
        path: "{{ juice_shop_dir }}"
        state: directory
        owner: ubuntu
        group: ubuntu
        mode: '0755'

    - name: Clone Juice Shop repository
      git:
        repo: https://github.com/juice-shop/juice-shop.git
        dest: "{{ juice_shop_dir }}"
        depth: 1
        update: yes
        force: yes
      become: true
      become_user: ubuntu

    - name: Ensure repo ownership is ubuntu
      file:
        path: "{{ juice_shop_dir }}"
        owner: ubuntu
        group: ubuntu
        recurse: yes

    - name: Create tracer.ts (DataDog tracer - MINIMAL VERSION)
      copy:
        dest: "{{ juice_shop_dir }}/tracer.ts"
        content: |
          // tracer.ts
          import tracer from 'dd-trace';
          tracer.init();
          export default tracer;
        owner: ubuntu
        group: ubuntu
        mode: '0644'

    - name: Create Dockerfile (YOUR WORKING VERSION)
      copy:
        dest: "{{ juice_shop_dir }}/Dockerfile"
        content: |
          # Multi-stage Dockerfile for Juice Shop with Security Tools
          FROM node:22-bookworm-slim AS builder

          RUN apt-get update && apt-get install -y git python3 make g++ && rm -rf /var/lib/apt/lists/*
          WORKDIR /juice-shop

          # Clone Juice Shop
          RUN git clone --depth 1 https://github.com/juice-shop/juice-shop.git . && rm -rf .git

          # Install ALL dependencies (needed for TypeScript compilation)
          RUN npm install

          # Install security tools
          RUN npm install --save-exact @aikidosec/firewall dd-trace

          # Copy tracer
          COPY tracer.ts ./tracer.ts

          # ‚úÖ CRITICAL: Modify TypeScript source files BEFORE compilation
          # 1. Modify app.ts (main entry point)
          RUN sed -i "1i import './tracer' // DataDog APM" app.ts && \
              sed -i "2i import '@aikidosec/firewall' // Aikido RASP" app.ts
          
          # 2. Also modify server.ts (in case it's loaded independently)
          RUN sed -i "1i import './tracer' // DataDog APM" server.ts && \
              sed -i "2i import '@aikidosec/firewall' // Aikido RASP" server.ts

          # Verify modifications
          RUN echo "=== Modified app.ts (first 10 lines) ===" && head -10 app.ts
          RUN echo "=== Modified server.ts (first 10 lines) ===" && head -10 server.ts

          # ‚úÖ COMPILE TypeScript to JavaScript (creates build/ directory)
          RUN npx tsc || npm run build:server || echo "TypeScript compilation attempted"
          
          # Verify build directory was created
          RUN ls -la build/ && \
              echo "=== build/app.js (first 30 lines) ===" && \
              head -30 build/app.js && \
              echo "=== Checking for tracer/aikido in build/app.js ===" && \
              grep -E "tracer|aikido" build/app.js | head -5

          # Build frontend (Angular)
          RUN cd frontend && npm install --legacy-peer-deps && \
              node ./node_modules/@angular/cli/bin/ng build --configuration production || echo "Frontend build completed"

          # --- Production stage ---
          FROM node:22-bookworm-slim
          
          RUN apt-get update && apt-get install -y curl procps && rm -rf /var/lib/apt/lists/*
          RUN groupadd -r juiceshop && useradd -r -g juiceshop juiceshop
          
          WORKDIR /juice-shop

          # Copy entire application including compiled build/ directory
          COPY --from=builder --chown=juiceshop:juiceshop /juice-shop /juice-shop

          # Ensure directories exist with correct permissions
          RUN mkdir -p /juice-shop/logs /juice-shop/ftp /juice-shop/data && \
              chown -R juiceshop:juiceshop /juice-shop

          USER juiceshop
          EXPOSE 3000

          HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
            CMD curl -f http://localhost:3000/rest/admin/application-version || exit 1

          ENV NODE_ENV=production \
              PORT=3000 \
              NODE_OPTIONS="--max-old-space-size=2048"
          
          # ‚úÖ This matches your local command: npm start (which runs: node build/app)
          CMD ["npm", "start"]
        owner: ubuntu
        group: ubuntu
        mode: '0644'

    - name: Create docker-compose.yml
      copy:
        dest: "{{ juice_shop_dir }}/docker-compose.yml"
        content: |
          services:
            juice-shop:
              build: .
              image: juice-shop-secure:latest
              container_name: juice-shop
              restart: unless-stopped
              ports:
                - "3000:3000"
              environment:
                # Node environment
                NODE_ENV: production
                
                # DataDog APM Configuration
                DD_ENV: dev
                DD_SERVICE: juice-shop
                DD_VERSION: latest
                DD_AGENT_HOST: 172.17.0.1
                DD_TRACE_AGENT_PORT: 8126
                
                # DataDog Tracing & Profiling
                DD_LOGS_INJECTION: "true"
                DD_TRACE_SAMPLE_RATE: "1"
                DD_PROFILING_ENABLED: "true"
                DD_RUNTIME_METRICS_ENABLED: "true"
                DD_DATA_STREAMS_ENABLED: "true"
                DD_TRACE_REMOVE_INTEGRATION_SERVICE_NAMES_ENABLED: "true"
                
                # DataDog Security (AppSec & IAST)
                DD_APPSEC_ENABLED: "true"
                DD_IAST_ENABLED: "true"
                
                # Aikido Configuration
                AIKIDO_TOKEN: ${AIKIDO_TOKEN}
                AIKIDO_BLOCK: "true"
                AIKIDO_DEBUG: "true"
                
              env_file:
                - .env
              extra_hosts:
                - "host.docker.internal:host-gateway"
              healthcheck:
                test: ["CMD", "curl", "-f", "http://localhost:3000"]
                interval: 30s
                timeout: 10s
                retries: 3
                start_period: 60s
              logging:
                driver: "json-file"
                options:
                  max-size: "10m"
                  max-file: "3"
        owner: ubuntu
        group: ubuntu
        mode: '0644'

    - name: Create .env file with secrets
      copy:
        dest: "{{ juice_shop_dir }}/.env"
        content: |
          # Aikido Zen RASP Configuration
          AIKIDO_TOKEN={{ aikido_token }}
          AIKIDO_BLOCK=true
          AIKIDO_DEBUG=true
          
          # DataDog Configuration
          DD_ENV=dev
          DD_VERSION=latest
        owner: ubuntu
        group: ubuntu
        mode: '0600'
      no_log: true

    - name: Create deploy.sh
      copy:
        dest: "{{ juice_shop_dir }}/deploy.sh"
        content: |
          #!/bin/bash
          set -e
          cd "{{ juice_shop_dir }}"
          
          echo "üöÄ Deploying Juice Shop with Security Instrumentation..."
          
          # Stop existing containers
          docker compose down || true
          
          # Remove old images
          docker rmi juice-shop-secure:latest 2>/dev/null || true
          
          # Build with progress output
          echo "üì¶ Building Docker image (this includes TypeScript compilation)..."
          docker compose build --no-cache --progress=plain
          
          # Start containers
          echo "‚ñ∂Ô∏è  Starting containers..."
          docker compose up -d
          
          # Wait for container to be healthy
          echo "‚è≥ Waiting for application to start..."
          sleep 45
          
          # Verify deployment
          if docker ps | grep -q juice-shop; then
            echo "‚úÖ Container is running"
            
            # Check instrumentation in COMPILED code
            echo ""
            echo "üîç Verifying Security Instrumentation (Compiled JS):"
            echo "======================================"
            docker exec juice-shop head -20 build/server.js | grep -E "tracer|aikido" || echo "‚ö†Ô∏è  Check build/server.js manually"
            
            # Check logs
            echo ""
            echo "üìä Application Logs:"
            docker logs juice-shop 2>&1 | tail -30
            
            # Test application
            if curl -f -s http://localhost:3000 > /dev/null; then
              echo ""
              echo "‚úÖ Application is accessible at http://localhost:3000"
              echo "‚úÖ Deployment successful!"
              echo ""
              echo "üîç Generate traffic to see traces in DataDog:"
              echo "   for i in {1..20}; do curl -s http://localhost:3000 >/dev/null; done"
            else
              echo "‚ö†Ô∏è  Application not responding yet (may need more time)"
            fi
          else
            echo "‚ùå Deployment failed - container not running"
            docker compose logs
            exit 1
          fi
        mode: '0755'

    - name: Create verification script
      copy:
        dest: "{{ juice_shop_dir }}/verify-instrumentation.sh"
        content: |
          #!/bin/bash
          echo "üîç Security Instrumentation Verification"
          echo "========================================"
          
          echo ""
          echo "üìã Container Status:"
          docker ps | grep juice-shop || echo "‚ùå Container not running"
          
          echo ""
          echo "üîß Environment Variables:"
          docker exec juice-shop env | grep -E "DD_|AIKIDO_" | sort
          
          echo ""
          echo "üìä Application Startup Logs (looking for AIKIDO and DataDog):"
          docker logs juice-shop 2>&1 | head -80 | grep -i -E "aikido|datadog|tracer|firewall|dd-trace|starting agent" || echo "‚ö†Ô∏è  Security tool logs not found"
          
          echo ""
          echo "üîç AIKIDO Startup Messages:"
          docker logs juice-shop 2>&1 | grep -i "AIKIDO:" | head -20
          
          echo ""
          echo "üì° DataDog Agent Status on Host:"
          sudo datadog-agent status | grep -A 10 "APM Agent" 2>/dev/null || echo "‚ö†Ô∏è  Run with sudo to see agent status"
        mode: '0755'

  handlers:
    - name: Restart DataDog Agent
      systemd:
        name: datadog-agent
        state: restarted

    - name: Restart Prometheus
      systemd:
        name: prometheus
        state: restarted

    - name: Restart Grafana
      systemd:
        name: grafana-server
        state: restarted

    - name: Restart Metrics Exporter
      systemd:
        name: security-metrics-exporter
        state: restarted