name: RASP vs IAST Comparative Benchmarking Pipeline (IMPROVED)

# This improved version integrates ZAP scans into the benchmark comparison
# as suggested - making it a true comprehensive comparison

on:
  push:
    branches: [main, dev, dev-test]
  pull_request:
    branches: [main, dev, dev-test]
  workflow_dispatch:
    inputs:
      skip_ansible:
        description: "Skip Ansible configuration"
        required: false
        type: boolean
        default: true
      test_configurations:
        description: "Test configurations to run"
        required: false
        type: choice
        default: "all"
        options:
          - all
          - baseline
          - iast-only
          - rasp-monitor
          - rasp-block
      include_zap_scan:
        description: "Include ZAP comprehensive scan (slower but thorough)"
        required: false
        type: boolean
        default: true
      zap_scan_depth:
        description: "ZAP scan depth"
        required: false
        type: choice
        default: "baseline"
        options:
          - baseline # Fast: ~5 min per config
          - full # Slow: ~20 min per config

env:
  EC2_INSTANCE_IP: ${{ secrets.EC2_INSTANCE_IP }}
  SEMGREP_APP_TOKEN: ${{ secrets.SEMGREP_APP_TOKEN }}
  EC2_SSH_PRIVATE_KEY: ${{ secrets.EC2_SSH_PRIVATE_KEY }}
  ANSIBLE_VAULT_PASSWORD: ${{ secrets.ANSIBLE_VAULT_PASSWORD }}

jobs:
  sast-semgrep:
    name: üîç SAST - Semgrep Security Scan
    runs-on: ubuntu-latest
    container:
      image: semgrep/semgrep
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Run Semgrep Scan
        run: |
          semgrep scan --config=auto --json --output=semgrep-results.json || true
          semgrep scan --config=auto --sarif --output=semgrep-results.sarif || true
        env:
          SEMGREP_APP_TOKEN: ${{ secrets.SEMGREP_APP_TOKEN }}

      - name: Upload to Semgrep Dashboard
        if: always()
        run: semgrep ci
        env:
          SEMGREP_APP_TOKEN: ${{ secrets.SEMGREP_APP_TOKEN }}
        continue-on-error: true

      - name: Upload SARIF to GitHub Security
        if: always()
        uses: github/codeql-action/upload-sarif@v3
        with:
          sarif_file: semgrep-results.sarif
          category: semgrep
        continue-on-error: true

      - name: Upload Semgrep Artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: semgrep-results
          path: |
            semgrep-results.json
            semgrep-results.sarif

  check-prerequisites:
    name: ‚úÖ Verify Prerequisites
    runs-on: ubuntu-latest
    needs: sast-semgrep
    outputs:
      can_proceed: ${{ steps.check.outputs.can_proceed }}
    steps:
      - name: Check Required Secrets
        id: check
        run: |
          echo "Checking required secrets..."
          MISSING_SECRETS=()
          if [ -z "${{ secrets.EC2_INSTANCE_IP }}" ]; then
            MISSING_SECRETS+=("EC2_INSTANCE_IP")
          fi
          if [ -z "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then
            MISSING_SECRETS+=("EC2_SSH_PRIVATE_KEY")
          fi
          if [ -z "${{ secrets.ANSIBLE_VAULT_PASSWORD }}" ]; then
            MISSING_SECRETS+=("ANSIBLE_VAULT_PASSWORD")
          fi
          if [ ${#MISSING_SECRETS[@]} -ne 0 ]; then
            echo "::error::Missing required secrets: ${MISSING_SECRETS[*]}"
            echo "can_proceed=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          echo "‚úÖ All required secrets are set"
          echo "can_proceed=true" >> $GITHUB_OUTPUT

  configure-instance:
    name: ‚öôÔ∏è Configure Instance with Ansible
    runs-on: ubuntu-latest
    needs: check-prerequisites
    if: |
      needs.check-prerequisites.outputs.can_proceed == 'true' &&
      (github.event.inputs.skip_ansible == 'false' || github.event.inputs.skip_ansible == '')
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Ansible
        run: pip install ansible boto3 botocore

      - name: Setup SSH Key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa

      - name: Wait for SSH Availability
        run: |
          echo "Testing SSH connection..."
          for i in {1..30}; do
            if ssh -o StrictHostKeyChecking=no -o ConnectTimeout=5 \
               -i ~/.ssh/id_rsa ubuntu@${{ env.EC2_INSTANCE_IP }} \
               "echo 'SSH ready'" 2>/dev/null; then
              echo "‚úÖ SSH connection successful"
              exit 0
            fi
            echo "Attempt $i/30: Waiting for SSH..."
            sleep 10
          done
          echo "::error::SSH connection timeout"
          exit 1

      - name: Create Ansible Inventory
        run: |
          mkdir -p ansible
          cat > ansible/inventory.ini <<EOF
          [juiceshop]
          ${{ env.EC2_INSTANCE_IP }} ansible_user=ubuntu ansible_ssh_private_key_file=~/.ssh/id_rsa ansible_ssh_common_args='-o StrictHostKeyChecking=no'
          EOF

      - name: Create Vault Password File
        run: |
          cd ansible
          echo "${{ secrets.ANSIBLE_VAULT_PASSWORD }}" > .vault_pass
          chmod 600 .vault_pass

      - name: Run Ansible Playbook
        run: |
          cd ansible
          ansible-playbook -i inventory.ini playbook.yml \
            --vault-password-file .vault_pass -v

      - name: Clean Up Vault Password
        if: always()
        run: rm -f ansible/.vault_pass

  setup-benchmarking:
    name: üõ†Ô∏è Verify Attack Tools
    runs-on: ubuntu-latest
    needs: [check-prerequisites, configure-instance]
    if: |
      always() &&
      needs.check-prerequisites.outputs.can_proceed == 'true' &&
      (needs.configure-instance.result == 'success' || needs.configure-instance.result == 'skipped')
    steps:
      - name: Setup SSH Key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H ${{ env.EC2_INSTANCE_IP }} >> ~/.ssh/known_hosts

      - name: Verify Attack Tools Installation
        run: |
          ssh -T -i ~/.ssh/id_rsa ubuntu@${{ env.EC2_INSTANCE_IP }} << 'ENDSSH'
            echo "üì¶ Verifying attack automation tools..."
            
            if [ -d "/opt/benchmark-tools" ]; then
              echo "‚úÖ Benchmark tools directory exists"
              ls -la /opt/benchmark-tools/
            else
              echo "‚ö†Ô∏è  Benchmark tools not found - run with skip_ansible: false"
            fi
            
            if command -v sqlmap &> /dev/null; then
              echo "‚úÖ SQLmap installed: $(sqlmap --version 2>&1 | head -1)"
            else
              echo "‚ö†Ô∏è  SQLmap not found"
            fi
            
            echo "‚úÖ Checking Python packages..."
            python3 -c "import requests; print('‚úÖ requests OK')" || echo "‚ö†Ô∏è  requests missing"
            python3 -c "import bs4; print('‚úÖ beautifulsoup4 OK')" || echo "‚ö†Ô∏è  bs4 missing"
            
            echo "‚úÖ Verification complete"
          ENDSSH

  benchmark-tests:
    name: üß™ ${{ matrix.config_name }}
    runs-on: ubuntu-latest
    needs: [check-prerequisites, setup-benchmarking]
    if: |
      always() &&
      needs.check-prerequisites.outputs.can_proceed == 'true' &&
      needs.setup-benchmarking.result == 'success'
    strategy:
      max-parallel: 1
      fail-fast: false
      matrix:
        include:
          - config: baseline
            config_name: "Configuration 1: Baseline (No Protection)"
            dd_appsec: "false"
            dd_iast: "false"
            aikido_block: "false"
            aikido_enabled: "false"
          - config: iast-only
            config_name: "Configuration 2: DataDog IAST Only"
            dd_appsec: "true"
            dd_iast: "true"
            aikido_block: "false"
            aikido_enabled: "false"
          - config: rasp-monitor
            config_name: "Configuration 3: Zen RASP (Monitoring)"
            dd_appsec: "false"
            dd_iast: "false"
            aikido_block: "false"
            aikido_enabled: "true"
          - config: rasp-block
            config_name: "Configuration 4: Zen RASP (Blocking)"
            dd_appsec: "false"
            dd_iast: "false"
            aikido_block: "true"
            aikido_enabled: "true"

    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Setup SSH Key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H ${{ env.EC2_INSTANCE_IP }} >> ~/.ssh/known_hosts

          cat >> ~/.ssh/config << 'SSHEOF'
          Host *
            ServerAliveInterval 60
            ServerAliveCountMax 30
            TCPKeepAlive yes
          SSHEOF

      - name: Deploy Application with ${{ matrix.config }} Configuration
        run: |
          cat > /tmp/deploy.sh << 'DEPLOYEOF'
          set -e
          cd /opt/juice-shop

          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
          echo "üöÄ Deploying: ${{ matrix.config_name }}"
          echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"

          docker compose down 2>/dev/null || true

          cat > .env << ENVEOF
          AIKIDO_TOKEN=${{ secrets.ZEN_FIREWALL_TOKEN }}
          AIKIDO_BLOCK=${{ matrix.aikido_block }}
          AIKIDO_DEBUG=true
          DD_ENV=${{ matrix.config }}
          DD_VERSION=latest
          ENVEOF

          cat > docker-compose.yml << COMPOSEEOF
          services:
            juice-shop:
              build: .
              image: juice-shop-secure:latest
              container_name: juice-shop
              restart: unless-stopped
              ports:
                - "3000:3000"
              environment:
                NODE_ENV: production
                DD_ENV: ${{ matrix.config }}
                DD_SERVICE: juice-shop
                DD_VERSION: latest
                DD_AGENT_HOST: 172.17.0.1
                DD_TRACE_AGENT_PORT: 8126
                DD_LOGS_INJECTION: "true"
                DD_TRACE_SAMPLE_RATE: "1"
                DD_PROFILING_ENABLED: "true"
                DD_RUNTIME_METRICS_ENABLED: "true"
                DD_APPSEC_ENABLED: "${{ matrix.dd_appsec }}"
                DD_IAST_ENABLED: "${{ matrix.dd_iast }}"
                AIKIDO_TOKEN: \${AIKIDO_TOKEN}
                AIKIDO_BLOCK: "${{ matrix.aikido_block }}"
                AIKIDO_DEBUG: "true"
                AIKIDO_ENABLED: "${{ matrix.aikido_enabled }}"
              env_file:
                - .env
              extra_hosts:
                - "host.docker.internal:host-gateway"
              healthcheck:
                test: ["CMD", "curl", "-f", "http://localhost:3000"]
                interval: 30s
                timeout: 10s
                retries: 3
                start_period: 60s
          COMPOSEEOF

          # Check if Dockerfile exists
          if [ ! -f Dockerfile ]; then
            echo "‚ùå ERROR: Dockerfile not found!"
            echo "This means Ansible didn't run. Set skip_ansible: false"
            ls -la /opt/juice-shop/
            exit 1
          fi

          if ! docker images | grep -q "juice-shop-secure.*latest"; then
            echo "üì¶ Building Docker image (10-15 min first time)..."
            echo "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ"
            
            # Run directly to see output
            if ! timeout 1200 docker compose build 2>&1 | tee /tmp/build.log; then
              echo ""
              echo "‚ùå Build failed or timed out"
              exit 1
            fi
            
            echo ""
            echo "‚úÖ Build completed"
          else
            echo "‚úÖ Using existing image"
          fi

          echo "‚ñ∂Ô∏è  Starting container..."
          docker compose up -d

          echo "‚è≥ Waiting for application..."
          sleep 60

          if ! docker ps | grep -q juice-shop; then
            echo "‚ùå Container failed to start"
            docker compose logs --tail=50
            exit 1
          fi

          for i in {1..10}; do
            if curl -f -s http://localhost:3000 > /dev/null; then
              echo "‚úÖ Application ready"
              echo ""
              echo "üìã Configuration:"
              echo "  - DataDog AppSec: ${{ matrix.dd_appsec }}"
              echo "  - DataDog IAST: ${{ matrix.dd_iast }}"
              echo "  - Aikido Enabled: ${{ matrix.aikido_enabled }}"
              echo "  - Aikido Blocking: ${{ matrix.aikido_block }}"
              exit 0
            fi
            echo "Waiting... ($i/10)"
            sleep 5
          done

          echo "‚ö†Ô∏è  Application may need more time"
          exit 0
          DEPLOYEOF

          scp -i ~/.ssh/id_rsa /tmp/deploy.sh ubuntu@${{ env.EC2_INSTANCE_IP }}:/tmp/deploy.sh
          ssh -T -i ~/.ssh/id_rsa ubuntu@${{ env.EC2_INSTANCE_IP }} "bash /tmp/deploy.sh"
          DEPLOY_EXIT=$?

          if [ $DEPLOY_EXIT -ne 0 ]; then
            echo "::error::Deployment failed with exit code $DEPLOY_EXIT"
            exit 1
          fi

          echo "‚úÖ Deployment successful"
        timeout-minutes: 35

      - name: Collect Performance Baseline
        run: |
          ssh -T -i ~/.ssh/id_rsa ubuntu@${{ env.EC2_INSTANCE_IP }} << 'ENDSSH'
            RESULTS_DIR="/tmp/benchmark-results/${{ matrix.config }}"
            mkdir -p "$RESULTS_DIR"
            
            if [ -x /opt/benchmark-tools/collect_metrics.sh ]; then
              /opt/benchmark-tools/collect_metrics.sh "${{ matrix.config }}" "$RESULTS_DIR"
            else
              echo "‚ö†Ô∏è  Metrics script not found, skipping"
            fi
          ENDSSH
        continue-on-error: true

      - name: Run Custom Attack Suite
        run: |
          ssh -T -i ~/.ssh/id_rsa ubuntu@${{ env.EC2_INSTANCE_IP }} << 'ENDSSH'
            RESULTS_DIR="/tmp/benchmark-results/${{ matrix.config }}"
            
            if [ -x /opt/benchmark-tools/run_all_attacks.sh ]; then
              /opt/benchmark-tools/run_all_attacks.sh \
                "http://localhost:3000" \
                "${{ matrix.config }}" \
                "$RESULTS_DIR"
            else
              echo "‚ö†Ô∏è  Attack scripts not found, skipping"
            fi
          ENDSSH
        continue-on-error: true

      # NEW: ZAP scan integrated into benchmark
      - name: Run ZAP Security Scan
        if: |
          github.event.inputs.include_zap_scan == 'true' ||
          github.ref == 'refs/heads/main'
        run: |
          ssh -T -i ~/.ssh/id_rsa ubuntu@${{ env.EC2_INSTANCE_IP }} << 'ENDSSH'
            RESULTS_DIR="/tmp/benchmark-results/${{ matrix.config }}"
            mkdir -p "$RESULTS_DIR"
            
            echo "üï∑Ô∏è  Running ZAP ${{ github.event.inputs.zap_scan_depth || 'baseline' }} scan..."
            
            SCAN_TYPE="${{ github.event.inputs.zap_scan_depth || 'baseline' }}"
            
            if [ "$SCAN_TYPE" = "baseline" ]; then
              TIMEOUT=300  # 5 minutes
              ZAP_CMD="zap-baseline.py"
            else
              TIMEOUT=1200  # 20 minutes
              ZAP_CMD="zap-full-scan.py"
            fi
            
            docker run --rm --network host \
              -v "$RESULTS_DIR:/zap/wrk:rw" \
              ghcr.io/zaproxy/zaproxy:stable \
              $ZAP_CMD \
              -t http://localhost:3000 \
              -r zap-report.html \
              -J zap-report.json \
              -w zap-report.md \
              -m 3 -T $TIMEOUT \
              || echo "ZAP scan completed with warnings"
            
            echo "‚úÖ ZAP scan completed"
          ENDSSH
        continue-on-error: true
        timeout-minutes: 35

      # NEW: Parse ZAP results for benchmark comparison
      - name: Parse ZAP Results for Benchmark
        if: |
          github.event.inputs.include_zap_scan == 'true' ||
          github.ref == 'refs/heads/main'
        run: |
          ssh -T -i ~/.ssh/id_rsa ubuntu@${{ env.EC2_INSTANCE_IP }} << 'ENDSSH'
            RESULTS_DIR="/tmp/benchmark-results/${{ matrix.config }}"
            
            if [ -f "$RESULTS_DIR/zap-report.json" ]; then
              python3 << 'PYEOF'
          import json
          import os

          results_dir = "/tmp/benchmark-results/${{ matrix.config }}"
          zap_file = f"{results_dir}/zap-report.json"

          if os.path.exists(zap_file):
              with open(zap_file, 'r') as f:
                  zap_data = json.load(f)
              
              # Count vulnerabilities by risk level
              alerts = zap_data.get('site', [{}])[0].get('alerts', [])
              
              high_risk = sum(1 for a in alerts if a.get('riskdesc', '').startswith('High'))
              medium_risk = sum(1 for a in alerts if a.get('riskdesc', '').startswith('Medium'))
              low_risk = sum(1 for a in alerts if a.get('riskdesc', '').startswith('Low'))
              info = sum(1 for a in alerts if a.get('riskdesc', '').startswith('Info'))
              
              total_alerts = len(alerts)
              
              # Estimate blocked vs detected
              # If high risk exists, they weren't blocked
              # This is a rough heuristic - ZAP doesn't directly report blocking
              blocked_estimate = 0 if high_risk > 0 else medium_risk + high_risk
              
              with open(f'{results_dir}/zap_parsed.csv', 'w') as f:
                  f.write('METRIC,VALUE\n')
                  f.write(f'TOTAL_ALERTS,{total_alerts}\n')
                  f.write(f'HIGH_RISK,{high_risk}\n')
                  f.write(f'MEDIUM_RISK,{medium_risk}\n')
                  f.write(f'LOW_RISK,{low_risk}\n')
                  f.write(f'INFO,{info}\n')
                  f.write(f'BLOCKED_ESTIMATE,{blocked_estimate}\n')
              
              print(f"‚úÖ ZAP Results Parsed:")
              print(f"   Total Alerts: {total_alerts}")
              print(f"   High Risk: {high_risk}")
              print(f"   Medium Risk: {medium_risk}")
              print(f"   Estimated Blocked: {blocked_estimate}")
          PYEOF
            else
              echo "‚ö†Ô∏è  ZAP report not found, skipping parse"
            fi
          ENDSSH
        continue-on-error: true

      - name: Collect Security Logs
        run: |
          ssh -T -i ~/.ssh/id_rsa ubuntu@${{ env.EC2_INSTANCE_IP }} << 'ENDSSH'
            RESULTS_DIR="/tmp/benchmark-results/${{ matrix.config }}"
            
            docker logs juice-shop > "$RESULTS_DIR/container.log" 2>&1 || true
            
            grep -i "aikido\|datadog\|blocked\|detected\|attack" \
              "$RESULTS_DIR/container.log" > "$RESULTS_DIR/security_events.log" 2>&1 || true
            
            BLOCKED=$(grep -c "blocked" "$RESULTS_DIR/security_events.log" 2>/dev/null || echo "0")
            DETECTED=$(grep -c "detected" "$RESULTS_DIR/security_events.log" 2>/dev/null || echo "0")
            
            echo "METRIC,COUNT" > "$RESULTS_DIR/security_events.csv"
            echo "BLOCKED,$BLOCKED" >> "$RESULTS_DIR/security_events.csv"
            echo "DETECTED,$DETECTED" >> "$RESULTS_DIR/security_events.csv"
          ENDSSH

      - name: Download Results
        run: |
          mkdir -p ./results/${{ matrix.config }}
          scp -i ~/.ssh/id_rsa -r \
            ubuntu@${{ env.EC2_INSTANCE_IP }}:/tmp/benchmark-results/${{ matrix.config }}/* \
            ./results/${{ matrix.config }}/ || echo "No results to download"

      - name: Upload Results Artifact
        uses: actions/upload-artifact@v4
        with:
          name: results-${{ matrix.config }}
          path: ./results/${{ matrix.config }}/
          retention-days: 30
        continue-on-error: true

  comparative-analysis:
    name: üìà Generate Comprehensive Comparative Analysis
    runs-on: ubuntu-latest
    needs: benchmark-tests
    if: always()
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4

      - name: Download All Results
        uses: actions/download-artifact@v4
        with:
          path: ./all-results
        continue-on-error: true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install Analysis Tools
        run: pip install pandas matplotlib seaborn tabulate

      - name: Generate Comprehensive Analysis
        run: |
          python3 << 'PYEOF'
          import pandas as pd
          from pathlib import Path
          import json

          configs = ['baseline', 'iast-only', 'rasp-monitor', 'rasp-block']
          results_dir = Path('./all-results')

          if not results_dir.exists():
              print("‚ö†Ô∏è  No results found. Creating placeholder report.")
              with open('summary_statistics.csv', 'w') as f:
                  f.write('Configuration,Status\n')
                  f.write('ALL,NO_RESULTS\n')
              with open('comparative_results.csv', 'w') as f:
                  f.write('Configuration,Status\n')
                  f.write('ALL,NO_RESULTS\n')
              exit(0)

          all_data = []

          for config in configs:
              config_dir = results_dir / f'results-{config}' / config
              if not config_dir.exists():
                  continue
              
              # Parse custom attack results
              summary_file = config_dir / 'summary.csv'
              if summary_file.exists():
                  try:
                      df = pd.read_csv(summary_file)
                      for _, row in df.iterrows():
                          if 'ATTACK_TYPE' in row and row.get('ATTACK_TYPE') == 'DURATION_SECONDS':
                              continue
                          all_data.append({
                              'Configuration': config,
                              'Source': 'Custom Scripts',
                              'Attack Type': row.get('ATTACK_TYPE', 'UNKNOWN'),
                              'Total Attacks': row.get('TOTAL', 0),
                              'Detected': row.get('DETECTED', 0),
                              'Blocked': row.get('BLOCKED', 0),
                              'Detection Rate': f"{((row.get('DETECTED', 0) + row.get('BLOCKED', 0)) / max(row.get('TOTAL', 1), 1)) * 100:.1f}%",
                              'Block Rate': f"{(row.get('BLOCKED', 0) / max(row.get('TOTAL', 1), 1)) * 100:.1f}%"
                          })
                  except Exception as e:
                      print(f"Error reading {summary_file}: {e}")
              
              # Parse ZAP results if available
              zap_parsed_file = config_dir / 'zap_parsed.csv'
              if zap_parsed_file.exists():
                  try:
                      zap_df = pd.read_csv(zap_parsed_file)
                      zap_dict = dict(zip(zap_df['METRIC'], zap_df['VALUE']))
                      
                      all_data.append({
                          'Configuration': config,
                          'Source': 'ZAP Scan',
                          'Attack Type': 'COMPREHENSIVE',
                          'Total Attacks': zap_dict.get('TOTAL_ALERTS', 0),
                          'Detected': zap_dict.get('HIGH_RISK', 0) + zap_dict.get('MEDIUM_RISK', 0),
                          'Blocked': zap_dict.get('BLOCKED_ESTIMATE', 0),
                          'Detection Rate': 'N/A',
                          'Block Rate': 'N/A'
                      })
                  except Exception as e:
                      print(f"Error reading ZAP results: {e}")

          if len(all_data) == 0:
              print("‚ö†Ô∏è  No data collected")
              with open('summary_statistics.csv', 'w') as f:
                  f.write('Configuration,Status\n')
                  f.write('ALL,NO_DATA\n')
              with open('comparative_results.csv', 'w') as f:
                  f.write('Configuration,Status\n')
                  f.write('ALL,NO_DATA\n')
          else:
              df = pd.DataFrame(all_data)
              df.to_csv('comparative_results.csv', index=False)
              
              # Generate summary with combined results
              summary = []
              for config in configs:
                  config_data = df[df['Configuration'] == config]
                  
                  if len(config_data) > 0:
                      # Custom scripts totals
                      custom_data = config_data[config_data['Source'] == 'Custom Scripts']
                      custom_total = custom_data['Total Attacks'].sum()
                      custom_detected = custom_data['Detected'].sum()
                      custom_blocked = custom_data['Blocked'].sum()
                      
                      # ZAP totals
                      zap_data = config_data[config_data['Source'] == 'ZAP Scan']
                      zap_total = zap_data['Total Attacks'].sum() if len(zap_data) > 0 else 0
                      zap_high = zap_data['Detected'].sum() if len(zap_data) > 0 else 0
                      
                      summary.append({
                          'Configuration': config,
                          'Custom Total': int(custom_total),
                          'Custom Blocked': int(custom_blocked),
                          'Custom Detection Rate': f"{((custom_detected + custom_blocked) / custom_total * 100):.1f}%" if custom_total > 0 else "N/A",
                          'ZAP Alerts': int(zap_total),
                          'ZAP High Risk': int(zap_high),
                          'Combined Score': f"{((custom_blocked / custom_total * 100) if custom_total > 0 else 0):.1f}%"
                      })
              
              if summary:
                  pd.DataFrame(summary).to_csv('summary_statistics.csv', index=False)
                  print("‚úÖ Analysis complete with combined results")
              else:
                  with open('summary_statistics.csv', 'w') as f:
                      f.write('Configuration,Status\n')
                      f.write('ALL,INSUFFICIENT_DATA\n')
          PYEOF

      - name: Generate Comprehensive Report
        run: |
          cat > BENCHMARK_REPORT.md << 'EOF'
          # üîí RASP vs IAST Comprehensive Benchmark Report

          **Pipeline Run:** #${{ github.run_number }}  
          **Date:** $(date '+%Y-%m-%d %H:%M:%S UTC')
          **ZAP Scan Included:** ${{ github.event.inputs.include_zap_scan || 'auto (main branch)' }}

          ## üìä Executive Summary

          This benchmark combines results from:
          1. **Custom Attack Scripts** - Focused, controlled attack testing
          2. **OWASP ZAP DAST** - Comprehensive automated security scanning

          EOF

          if [ -f summary_statistics.csv ] && [ $(wc -l < summary_statistics.csv) -gt 1 ]; then
            python3 << 'PYEOF' >> BENCHMARK_REPORT.md || echo "_No data_" >> BENCHMARK_REPORT.md
          import pandas as pd
          try:
              df = pd.read_csv('summary_statistics.csv')
              if len(df) > 0:
                  print("\n## Configuration Comparison\n")
                  print(df.to_markdown(index=False))
          except: pass
          PYEOF
          else
            echo "‚ö†Ô∏è  No benchmark results available" >> BENCHMARK_REPORT.md
          fi

          cat >> BENCHMARK_REPORT.md << 'EOF'

          ## üìñ Interpretation Guide

          ### Custom Scripts
          - **Precision testing**: Known attacks with exact pass/fail measurement
          - Fast execution (~30 seconds)
          - 22 total attacks across SQL injection, XSS, and path traversal

          ### ZAP DAST
          - **Comprehensive discovery**: Tests hundreds of attack patterns
          - Slower execution (5-20 minutes depending on depth)
          - Industry-standard compliance reporting

          ### Combined Score
          Weights custom script blocking rate as primary metric with ZAP providing 
          comprehensive validation and discovery of additional vulnerabilities.

          EOF

          cat BENCHMARK_REPORT.md >> $GITHUB_STEP_SUMMARY

      - name: Upload Comprehensive Report
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-benchmark-report
          path: |
            BENCHMARK_REPORT.md
            comparative_results.csv
            summary_statistics.csv
          retention-days: 90
        continue-on-error: true

  # NEW: Deploy and start monitoring stack
  deploy-monitoring:
    name: üìä Deploy Monitoring Stack (Prometheus + Grafana)
    runs-on: ubuntu-latest
    needs: comparative-analysis
    if: always()
    steps:
      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H ${{ env.EC2_INSTANCE_IP }} >> ~/.ssh/known_hosts

      - name: Start Monitoring Stack
        run: |
          ssh -T -i ~/.ssh/id_rsa ubuntu@${{ env.EC2_INSTANCE_IP }} << 'ENDSSH'
            echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
            echo "üìä Starting Monitoring Stack"
            echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
            
            # Check if monitoring is already configured
            if [ ! -f /opt/monitoring/start_monitoring.sh ]; then
              echo "‚ùå Monitoring not configured. Run Ansible with skip_ansible: false"
              exit 1
            fi
            
            # Start monitoring
            cd /opt/monitoring
            ./start_monitoring.sh
            
            # Wait for services
            sleep 10
            
            # Verify services are running
            if docker ps | grep -q prometheus && docker ps | grep -q grafana; then
              echo "‚úÖ Monitoring stack is running"
              echo ""
              echo "üìä Access Points:"
              echo "   Grafana:    http://$(curl -s ifconfig.me):3001"
              echo "   Prometheus: http://$(curl -s ifconfig.me):9090"
              echo ""
              echo "   Login: admin / rasp_vs_iast_2024"
              echo ""
              echo "‚è∞ Monitoring will run for 2 hours to collect data"
              echo "   Then auto-stop to save costs"
            else
              echo "‚ùå Monitoring failed to start"
              exit 1
            fi
          ENDSSH

      - name: Create Monitoring Info Comment
        uses: actions/github-script@v7
        if: github.event_name == 'pull_request'
        with:
          script: |
            const ec2Ip = process.env.EC2_INSTANCE_IP;
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## üìä Monitoring Stack Deployed
              
              Real-time dashboards are now available:
              
              - **Grafana**: http://${ec2Ip}:3001
              - **Prometheus**: http://${ec2Ip}:9090
              
              **Login Credentials:**
              - Username: \`admin\`
              - Password: \`rasp_vs_iast_2024\`
              
              **Dashboard:** Navigate to Dashboards ‚Üí RASP vs IAST Security Benchmark
              
              ‚è∞ Monitoring will run for 2 hours, then auto-stop to save costs.
              
              üí° To stop manually: SSH into EC2 and run \`/opt/monitoring/stop_monitoring.sh\``
            });
        continue-on-error: true

  cleanup:
    name: üßπ Cleanup
    runs-on: ubuntu-latest
    needs: [benchmark-tests, comparative-analysis, deploy-monitoring]
    if: always()
    steps:
      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H ${{ env.EC2_INSTANCE_IP }} >> ~/.ssh/known_hosts

      - name: Clean Up
        run: |
          ssh -T -i ~/.ssh/id_rsa ubuntu@${{ env.EC2_INSTANCE_IP }} << 'ENDSSH'
            rm -rf /tmp/benchmark-results /tmp/deploy.sh
            echo "‚úÖ Cleanup complete"
            echo ""
            echo "‚ÑπÔ∏è  Monitoring stack is still running"
            echo "   Access: http://$(curl -s ifconfig.me):3001"
            echo "   To stop: /opt/monitoring/stop_monitoring.sh"
          ENDSSH
        continue-on-error: true
        # Add this job to your workflow after the existing ones

  deploy-continuous-benchmark:
    name: üöÄ Deploy Continuous Benchmark System
    runs-on: ubuntu-latest
    needs: [check-prerequisites, configure-instance]
    if: |
      github.ref == 'refs/heads/main' &&
      needs.check-prerequisites.outputs.can_proceed == 'true'
    steps:
      - name: Setup SSH Key
        run: |
          mkdir -p ~/.ssh
          echo "${{ secrets.EC2_SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H ${{ env.EC2_INSTANCE_IP }} >> ~/.ssh/known_hosts

      - name: Deploy and Start Continuous System
        run: |
          ssh -T -i ~/.ssh/id_rsa ubuntu@${{ env.EC2_INSTANCE_IP }} << 'ENDSSH'
            echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
            echo "üöÄ Deploying Continuous Benchmark System"
            echo "‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê"
            
            # Start the complete system
            if [ -f /opt/benchmark-tools/start_benchmark_system.sh ]; then
              /opt/benchmark-tools/start_benchmark_system.sh
            else
              echo "‚ùå Start script not found. Run Ansible first."
              exit 1
            fi
            
            # Wait for stabilization
            sleep 30
            
            # Check status
            /opt/benchmark-tools/check_status.sh
          ENDSSH

      - name: Create Summary Report
        run: |
          PUBLIC_IP="${{ env.EC2_INSTANCE_IP }}"

          cat >> $GITHUB_STEP_SUMMARY << EOF
          # üöÄ Continuous Benchmark System Deployed

          ## üìä Access Points

          | Service | URL | Credentials |
          |---------|-----|-------------|
          | **Grafana Dashboard** | [http://${PUBLIC_IP}:3001](http://${PUBLIC_IP}:3001) | admin / rasp_vs_iast_2024 |
          | **Prometheus** | [http://${PUBLIC_IP}:9090](http://${PUBLIC_IP}:9090) | None required |
          | **Metrics Endpoint** | [http://${PUBLIC_IP}:8000/metrics](http://${PUBLIC_IP}:8000/metrics) | None required |

          ## ‚öôÔ∏è System Configuration

          - **Rotation Interval**: Each configuration runs for 60 minutes
          - **Attack Frequency**: Attacks execute every 5 minutes
          - **Data Retention**: 30 days
          - **Configurations Tested**:
            1. Baseline (No Protection)
            2. DataDog IAST Only
            3. Aikido RASP (Monitoring)
            4. Aikido RASP (Blocking)

          ## üìà Key Metrics Being Collected

          - Blocking Effectiveness Rate
          - Detection Effectiveness Rate
          - Attack Success Rate
          - Response Time Distribution
          - False Positive Rate
          - RASP vs IAST Comparative Advantage
          - Security Score Trends

          ## üîß Management Commands

          \`\`\`bash
          # Check system status
          ssh ubuntu@${PUBLIC_IP} /opt/benchmark-tools/check_status.sh

          # View live logs
          ssh ubuntu@${PUBLIC_IP} journalctl -u continuous-benchmark -f

          # Restart benchmark service
          ssh ubuntu@${PUBLIC_IP} sudo systemctl restart continuous-benchmark

          # Stop system (preserve data)
          ssh ubuntu@${PUBLIC_IP} sudo systemctl stop continuous-benchmark
          \`\`\`

          ## üìä Expected Results Timeline

          - **15 minutes**: First attack results visible
          - **1 hour**: First configuration rotation
          - **4 hours**: Complete cycle through all configurations
          - **24 hours**: Sufficient data for trend analysis
          - **7 days**: Comprehensive comparative analysis ready

          ---

          ‚è∞ **Note**: System will run continuously. Monitor AWS costs and stop when sufficient data is collected.
          EOF

      - name: Setup Automatic Daily Report (Optional)
        if: github.event.inputs.enable_daily_reports == 'true'
        run: |
          ssh -T -i ~/.ssh/id_rsa ubuntu@${{ env.EC2_INSTANCE_IP }} << 'ENDSSH'
            # Create daily report generator
            cat > /opt/benchmark-tools/generate_daily_report.py << 'PYEOF'
          #!/usr/bin/env python3
          import json
          import pandas as pd
          from pathlib import Path
          from datetime import datetime, timedelta
          import smtplib
          from email.mime.text import MIMEText
          from email.mime.multipart import MIMEMultipart

          def generate_report():
              results_base = Path('/var/lib/benchmark-results')
              report = []
              
              configs = ['baseline', 'iast-only', 'rasp-monitor', 'rasp-block']
              
              for config in configs:
                  config_path = results_base / config
                  if not config_path.exists():
                      continue
                  
                  # Get last 24 hours of data
                  cutoff = datetime.now() - timedelta(hours=24)
                  total_attacks = 0
                  total_blocked = 0
                  total_detected = 0
                  
                  for result_dir in config_path.iterdir():
                      if result_dir.name == 'latest':
                          continue
                      
                      try:
                          timestamp = datetime.strptime(result_dir.name, '%Y%m%d_%H%M%S')
                          if timestamp > cutoff:
                              summary_file = result_dir / 'summary.json'
                              if summary_file.exists():
                                  with open(summary_file) as f:
                                      data = json.load(f)
                                  
                                  for attack_type, stats in data.get('attacks', {}).items():
                                      total_attacks += stats.get('total', 0)
                                      total_blocked += stats.get('blocked', 0)
                                      total_detected += stats.get('detected', 0)
                      except:
                          continue
                  
                  if total_attacks > 0:
                      report.append({
                          'Configuration': config,
                          'Total Attacks (24h)': total_attacks,
                          'Blocked': total_blocked,
                          'Detection Rate': f"{((total_detected + total_blocked) / total_attacks * 100):.1f}%",
                          'Block Rate': f"{(total_blocked / total_attacks * 100):.1f}%"
                      })
              
              return pd.DataFrame(report)

          if __name__ == '__main__':
              df = generate_report()
              print("Daily Security Benchmark Report")
              print("="*50)
              print(df.to_string(index=False))
              
              # Save to file
              df.to_csv('/tmp/daily_report.csv', index=False)
              print(f"\nReport saved to /tmp/daily_report.csv")
          PYEOF
            
            chmod +x /opt/benchmark-tools/generate_daily_report.py
            
            # Add to crontab for daily execution
            (crontab -l 2>/dev/null; echo "0 0 * * * /opt/benchmark-tools/generate_daily_report.py > /var/log/daily_report.log 2>&1") | crontab -
          ENDSSH
